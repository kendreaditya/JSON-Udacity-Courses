{
  "data": {
    "lesson": {
      "id": 696464,
      "key": "a58a21bd-9920-4439-bc99-a0d050a8cb83",
      "title": "Project: Generate TV Scripts",
      "semantic_type": "Lesson",
      "is_public": true,
      "version": "1.0.0",
      "locale": "en-us",
      "summary": "Generate a TV script by defining and training a recurrent neural network.",
      "lesson_type": "Classroom",
      "display_workspace_project_only": false,
      "resources": {
        "files": [
          {
            "name": "Videos Zip File",
            "uri": "https://zips.udacity-data.com/a58a21bd-9920-4439-bc99-a0d050a8cb83/696464/1540245105480/Project%3A+Generate+TV+Scripts+Videos.zip"
          },
          {
            "name": "Transcripts Zip File",
            "uri": "https://zips.udacity-data.com/a58a21bd-9920-4439-bc99-a0d050a8cb83/696464/1540245103632/Project%3A+Generate+TV+Scripts+Subtitles.zip"
          }
        ],
        "google_plus_link": null,
        "career_resource_center_link": null,
        "coaching_appointments_link": null,
        "office_hours_link": null,
        "aws_provisioning_link": null
      },
      "project": {
        "key": "03b3aefc-60c7-46f8-85d5-aa774a0ca0ec",
        "version": "1.0.0",
        "locale": "en-us",
        "duration": 30240,
        "semantic_type": "Project",
        "title": "Generate TV Scripts",
        "description": "# Generate TV Scripts\n\n### Introduction\nIn this project, you'll generate your own Seinfeld TV scripts using RNNs. You'll be using a Seinfeld dataset of scripts from 9 seasons. The Neural Network you'll build will generate a new, \"fake\" TV script.\n\n### Getting the project files\n\nThe project files can be found in our [public GitHub repo](https://github.com/udacity/deep-learning-v2-pytorch), in the `project-tv-script-generation` folder. You can download the files from there, but it's better to clone the repository to your computer using:\n\n```bash\ngit clone https://github.com/udacity/deep-learning-v2-pytorch.git\n```\n\nThis way you can stay up to date with any changes we make by pulling the changes to your local repository with `git pull`.\n\nYou may also choose to complete your project using the provided, in-classroom project notebook; this will just require that you run the project notebook, and download the complete files, as specified below.\n\n### Submission\n1. Ensure you've passed all the unit tests in the notebook.\n2. Ensure you pass all points on [the rubric](https://review.udacity.com/#!/rubrics/2260/view).\n3. When you're done with the project, please save the notebook as both an .ipynb file and as an HTML file. You can do this by going to the **File** menu in the notebook and choosing \"Download as\" > HTML. ** Ensure you submit both the Jupyter Notebook and it's HTML version together. **\n4. Package the \"dlnd_tv_script_generation.ipynb\", \"helper.py\", \"problem_unittests.py\", and the HTML file into a zip archive, or push the files from your GitHub repo.\n5. Hit Submit Project to submit your final zip file!\n\n### Advanced Projects\nAfter completing this project, try applying what you learned to one of these problems.\n- Generate your own Bach music using like [DeepBach](https://arxiv.org/pdf/1612.01010.pdf).\n- Predict seizures in intracranial EEG recordings on [Kaggle](https://www.kaggle.com/c/seizure-prediction).\n\n## Project Submission Checklist\n\n**Before submitting your project, please review and confirm the following items.** \n<input type=\"checkbox\"> I am confident all rubric items have been met and my project will pass as submitted.\n<input type=\"checkbox\"> Project builds correctly without errors and runs.\n<input type=\"checkbox\"> All required functionality exists and my project behaves as expected per the project's specifications.\n\n**Once you have checked all these items, you are ready to submit!**\n",
        "is_public": true,
        "summary": null,
        "forum_path": "",
        "rubric_id": "2260",
        "terminal_project_id": null,
        "resources": null,
        "image": {
          "url": "https://s3.amazonaws.com/video.udacity-data.com/topher/2017/March/58d2e596_project-3-hero2/project-3-hero2.jpg",
          "width": 1200,
          "height": 900
        }
      },
      "lab": null,
      "concepts": [
        {
          "id": 288313,
          "key": "b7ff74a2-223a-4d91-99ca-dabe031f7496",
          "title": "Introduction",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "b7ff74a2-223a-4d91-99ca-dabe031f7496",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 288314,
              "key": "5ae2d375-8ca2-46ea-bc73-afba21c7c5cf",
              "title": "Project-3-Intro",
              "semantic_type": "VideoAtom",
              "is_public": true,
              "instructor_notes": "",
              "video": {
                "youtube_id": "qNpv7IjQzo0",
                "china_cdn_id": "qNpv7IjQzo0.mp4"
              }
            }
          ]
        },
        {
          "id": 752766,
          "key": "8b9a94f1-2edf-4564-8b1b-643bcfdd5d8e",
          "title": "GPU Workspaces: Best Practices",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "8b9a94f1-2edf-4564-8b1b-643bcfdd5d8e",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 752768,
              "key": "1289f37b-527e-4490-8a23-f56724130b07",
              "title": null,
              "semantic_type": "TextAtom",
              "is_public": true,
              "text": "## Best Practices\n---\nThe following project iterates over a large amount of data, and it's expected to take a number of hours to train, even on GPU. Follow the best practices outlined below to avoid common issues with GPU Workspaces.\n\n- ### Keeping your connection alive during long processes\nWorkspaces automatically disconnect when the connection is inactive for about 30 minutes, which includes inactivity while deep learning models are training. You can use the workspace_utils.py module [here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5b0dea96_workspace-utils/workspace-utils.py) to keep your connection alive during training. The module provides a context manager and an iterator wrapper—see example use below.\n<br><br>\n**NOTE:** The script sometimes raises a connection error if the request is opened too frequently; just restart the Jupyter kernel & run the cells again to reset the error.\n<br><br>\n**NOTE:** These scripts will keep your connection alive while the training process is _running_, but the workspace will still disconnect 30 minutes after the last notebook cell finishes. Modify the notebook cells to **save your work** at the end of the last cell or else you'll lose all progress when the workspace terminates.\n\n**Example using context manager:**\n```python\nfrom workspace_utils import active_session\n\nwith active_session():\n    # do long-running work here\n```\n\n**Example using iterator wrapper:**\n```python\nfrom workspace_utils import keep_awake\n\nfor i in keep_awake(range(5)):\n    # do iteration with lots of work here\n\n```\n\n- ### Manage your GPU time\nIt is important to avoid wasting GPU time in Workspace projects that have GPU acceleration enabled. The benefits of GPU acceleration are most useful when evaluating deep learning models—especially during _training_. In most cases, you can build and test your model (including data pre-processing, defining model architecture, etc.) in CPU mode, then activate GPU mode to accelerate training.\n\n- ### Handling \"Out of Memory\" errors\nThis issue isn't specific to Workspaces, but rather it is an apparent issue between PyTorch & Jupyter, where Jupyter reports \"out of memory\" after a cell crashes. Jupyter holds references to active objects as long as the kernel is running—including objects created before an error is raised. This can cause Jupyter to persist large objects in memory long after they are no longer required. The only known solutions are:\n> * To reduce the `batch_size` of your data\n>* Reset the kernel and run the notebook cells again\n\n",
              "instructor_notes": ""
            }
          ]
        },
        {
          "id": 730882,
          "key": "a5033352-0624-4b36-9657-d6ae605faf9f",
          "title": "Project Workspace: TV Script Generation",
          "semantic_type": "Concept",
          "is_public": true,
          "user_state": {
            "node_key": "a5033352-0624-4b36-9657-d6ae605faf9f",
            "completed_at": null,
            "last_viewed_at": null,
            "unstructured": null
          },
          "resources": null,
          "atoms": [
            {
              "id": 751828,
              "key": "212eed99-c699-4438-a4e7-100a4e1e481e",
              "title": null,
              "semantic_type": "WorkspaceAtom",
              "is_public": true,
              "workspace_id": "viewkgsf5gl3qs",
              "pool_id": "jupytergpu",
              "view_id": "jupyter-gxkvo855k9o",
              "gpu_capable": true,
              "configuration": {
                "id": "reserved",
                "blueprint": {
                  "conf": {
                    "disk": null,
                    "ports": [],
                    "allowGrade": false,
                    "allowSubmit": false,
                    "defaultPath": "/notebooks/dlnd_tv_script_generation.ipynb"
                  },
                  "kind": "jupyter"
                },
                "workspaceId": "reserved"
              },
              "starter_files": null
            }
          ]
        }
      ]
    }
  }
}